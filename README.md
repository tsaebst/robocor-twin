# RoboCourier

Кінцевою метою є калібрування параметрів цифрового двійника середовища RoboCourier за допомогою RL-калібратора та порівняльних евристичних бейслайнів. 

Мета: навчити агента **адаптивно оновлювати оцінки параметрів твіна**, щоб мінімізувати похибку моделі та витрати на спостереження.

---

## Тета-параметри
Калібратор працює з вектором параметрів `theta`твіна:

- **Вартісні/нагородні параметри**: `step_cost`, компоненти reward shaping.
- **Динаміка руху / стохастика**: імовірності помилки/ковзання `p_slip`
- **батарея**: `battery_max`, витрати батареї на крок/дію, правила розряду/відновлення.
- **Умови завершення епізоду**: правила `terminated/truncated`, що залежать від внутрішніх гіперпараметрів.
---

## Структура репозиторію

- `train_controller_ppo.py` — **основний тренувальний  скрипт RL-калібратора через PPO**:  середовище, обгортки, логування, запуск навчання/оцінки.
- `plots/` — **ноутбуки/скрипти для візуалізацій**:
  - траєкторії оцінок `theta_est` у часі,
  - порівняння train/eval,
  - якість калібрування по компонентах (pos/bat/rew/term/slip),
  - витрати на запити (k_used, query-cost), ефективність стратегії запитів.
- `src/rc_calib/` — утиліти калібрування: параметризація `Theta`, застосування `theta` до твіна, обгортки дій/спостережень, метрики.

---

##процес RL-калібрування

### 1)twin & env
- **Twin** — локальне середовище з параметрами `theta`
- **Env (EC2)** — ground-truth для частини стану/перехідних величин у вибрані моменти часу.  В мене він запущений і працюючий на ЕС2 на AWS. При навчанні/калібруванні доступаюсь з локалу на нього.

### 2) агент на кожному кроці
На кожному кроці агент формує дію, яка зазвичай містить:
- **Query-рішення**: робити запит до oracle чи ні (`do_query ∈ {0,1}`).
- (Опційно) **Оновлення/дельту theta** або вибір режиму оновлення 

Після кроку:
- twin робить перехід з урахуванням поточної `theta`,
- якщо `do_query=1` і дозволено бюджетом/частотою - отримуємо oracle-дані,
- рахуємо втрати/метрики між прогнозом твіна і oracle,
- з цього формується **reward ** .

### 3)reward
reward складається з:
- **похибок прогнозу** які треба зменшувати;
- **штрафу за часті запити** / query cost, якого треба уникати без потреби;
- **штрафу за крок** / step penalty
- (option.) регуляризації на величину/частоту змін `theta`.
---

## Важливі нюанси 
 для рл-калібрування:
1. **Оптимальна стратегія запиті. має навчитись:
   - коли інформація критично потрібна
   - коли можна обійтись без запиту


2. **Різна каліброванісит параметрів**  
   Деякі параметри впливають на траєкторію сильно і швидко, інші слабко. Політика має навчитися не витрачати бюджет на майже неідентифіковані компоненти або робити це рідше.
тощо..
---

## Візуалізації та аналіз

У `plots/` зібрані ноутбуки/скрипти для аналізу:
- динаміки `theta_est` та збіжності,
- розкладу помилок по компонентах (pos/bat/rew/term/slip),
- використання бюджету запитів,
- порівняння PPO-калібратора з евристичними бейслайнами.
